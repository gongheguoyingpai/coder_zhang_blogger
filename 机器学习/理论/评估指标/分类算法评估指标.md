# 分类算法评估指标

根据分类器对数据的分类结果可以分为四种情况:
| 标记 | 说明 |
|----- |----|
|TP(True Positive)    | 将正样本划分为正的样本数|
|TN(True Negative)    | 将负样本划分为负的样本数|
|FP(False Positive)   | 将负样本划分为正的样本数|
|FN(False Negative)   | 将正样本划分为负的样本数|

### 准确率(Accuracy)
正确率反映的是将类别划分正确的比例，计算公式为:
$$
Accuracy = \frac{TP + TN}{TP+TN+FP+FN}
$$
### 精确率(Precision)
精确率反映的是预测器预测为正的样本中真的为正的比例，计算公式为:
$$
Precision = \frac{TP}{TP+FP}
$$
### 召回率(Recall)
召回率反映的是预测器将正样本识别为正的比例，计算公式为:
$$
Recall = \frac{TP}{TP+FN}
$$
### F1-Score
因为精确率和召回率两者之间是相互矛盾的，一般来讲在实际的模型中精确率越高，召回率就越低，反之亦然，F1-Score就是平衡精确率和召回率的指标。F1-Score的计算公式为:
$$
\frac{1}{F1-Score}=0.5 * (\frac{1}{Precision} + \frac{1}{Recall}) 
$$
### Roc和AUC
Roc和AUC一般用于模型通过输出一个实值或者概率值的情况，在分类器中，根据判断输出的实值或者概率值是否高于给定阈值判断分类的正负。在理想的状态下，对所有的正例分类器输出的值应该都是大于负例的。因此，根据分类器输出值，从大到小进行排序，然后从大到小开始扫描所有样本，**将每个样本看做正例**，当每个样本加进来之后计算两个值:
- 真正例率 
 $$
 TPR = \frac{TP}{TP+FN}
 $$

- 假正例率
$$
FPR = \frac{FP}{TN+FP}
$$
通过不断加入新样本进来，会组成一个序列[($TPR_1$, $FPR_1$), ($TPR_1$, $FPR_1$), ..., ($TPR_n$, $FPR_n$)]，以序列中每个对的TPR值作为纵坐标，以序列中每个对的FPR值作为横坐标在坐标轴上做点并连线，即可生成ROC曲线。

通过ROC曲线进行两个模型对比时，如果A模型的ROC曲线完全在B模型的下面则可以认为B模型的效果要优于A模型，但是如果对于两个模型存在ROC曲线交叉的情况，此时就难以直观判断两个模型孰优孰劣，因此，此时就可以通过计算AUC来进行比较，AUC对应的就是作出的ROC曲线下面的面积。其计算方式就是采用微积分的方法，通过将ROC区分切分为一个个小的梯形，通过求小梯形的面积之和求得AUC的值，计算公式如下:
$$
AUC = 0.5 * \sum_{i=1}^{m-1}(x_{i+1}-x_{i})(y_i+y_{i+1})
$$
公式中的横纵坐标的值对应的就是上面用于绘制ROC曲线的序列的值。